---
title: "Convergencia del muestrador de Gibbs"
subtitle: "Basado en Cassella, George (1992)"
author: "Diego Uriarte"
date: "24/9/2018"
output: 
  pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(dplyr)
library(truncdist)
library(grid)
library(gridExtra)
library(knitr)    # For knitting document and include_graphics function
set.seed(14021991)
```
<!--Text comment
## ¿Qué es el muestrador de Gibbs?

Nos permite muestrar de una distribución sin tener que calcular la densidad conjunta. Está basada en cadenas de Markov. Por ejemplo, si queremos la marginal de x, deberíamos conocer la conjunta, y luego integrar las variables distintas de x:

$$ f(x) = \int \cdots \int f(x, y_1, \cdots, y_p) dy_1 \cdots dy_p $$

Se debería hallar la marginal analítica o numéricamente, pero a veces es díficil o imposible. En esos casos podemos usar el muestrador de Gibbs que es un algoritmo para obtener muestrar de la marginal de una variable a partir de las condicionales. 
-->

## ¿Cómo funciona el muestrador de Gibbs? 

Ilustraremos con un ejemplo bivariado. Queremos muestras de $f(x)$ (distribuida de manera conjunta con $y$). Conocemos las condicionales $f(x|y)$ y $f(y|x)$. Se especifica un valor inicial para $Y'_0 = y'_0$. Se obtiene apartir de las condicionales una sucesión de muestras para x e y:

$$
X'_j \sim f(x|Y'_j = y'_j)
$$
$$
Y'_{j+1} \sim f(y|X'_j = x'_j)
$$

Así, obtenemos una secuencia: $Y'_0, X'_0, Y'_1, X'_1,\cdots Y'_k, X'_k,$. Resulta que bajo algunas condiciones generales, cuando $k \rightarrow \infty$, $X'_k$ es efectivamente una muestra de $f(x)$

## Una prueba simple de la convergencia del muestrador de Gibbs (caso discreto)

Mostraremos como converge el muestreo tomando como ejemplo una distribución bivariada, donde X e Y se distribuyen Bernoulli de manera conjunta.

La distribución conjunta de X, Y, además de la marginal de X están dadas por:

\[
\begin{bmatrix}f_{x,y}(0,0) & f_{x,y}(1,0)\\
f_{x,y}(0,1) & f_{x,y}(1,1)
\end{bmatrix}=\begin{bmatrix}p_{1} & p_{2}\\
p_{\text{3}} & p_{4}
\end{bmatrix}
| f_x = \begin{bmatrix} p_1 + p_3 & p_2 + p_4 \end{bmatrix}
\]

Determinamos las probabilidades condicionales $X|Y = y$ e $Y|X = x$, formulando las siguientes matrices:

\[
A_{y|x}=  \begin{bmatrix} \frac{p_1}{p_1+p_3} & \frac{p_3}{p_1+p_3}\\
                \frac{p_2}{p_2+p_4}& \frac{p_4}{p_2+p_4}
          \end{bmatrix}
A_{x|y} = \begin{bmatrix} \frac{p_1}{p_1+p_2} & \frac{p_2}{p_1+p_2}\\
                \frac{p_3}{p_3+p_4}& \frac{p_4}{p_3+p_4}
           \end{bmatrix}
\]



Dado que estamos interesados en muestrar de la marginal de X, nos interesan los valores $X'_0, X'_1, X'_2, \cdots$. Sin embargo, para ir de $X'_0 \rightarrow X'_1$ debemos  muestrar $Y'_1$, es decir, $X'_0 \rightarrow Y'_1 \rightarrow X'_1$. 


$$
P(X'_1 = x_1 |X'_0 = x_0) = \sum_y P(X'_1 = x_1 |Y'_1 = y) \times P(Y'_1 = y |X'_0 = x_0) 
$$


Para este caso en particular, se tiene que la matrices de transición para x está dada por $[A_{x|x} = A_{y|x} A_{x|y}$:

Ahora, es directo calcular la distribución marginal de realización k, $X'_k$:

$f_k = f_0 A^k_{x|x} = (f_0 A^{k-1}_{x|x})A_{x|x} = f_{k-1}A_{x|x}$


Bajo ciertas condiciones (matriz con elementos positivos), se tiene un punto fijo tal que: $f = f A_{x|x}$:

En nuestro caso simple:


$$
\begin{aligned}
f_x A_{x|x} =  f_x A_{y|x} A_{x|y} &= \begin{bmatrix}
                   p_1 + p_3 & p_2 + p_4
                \end{bmatrix}
                \begin{bmatrix} 
                  \frac{p_1}{p_1+p_3} & \frac{p_3}{p_1+p_3}\\
                  \frac{p_2}{p_2+p_4}& \frac{p_4}{p_2+p_4}
                \end{bmatrix} 
                \begin{bmatrix} 
                  \frac{p_1}{p_1+p_2} & \frac{p_2}{p_1+p_2}\\
                  \frac{p_3}{p_3+p_4}& \frac{p_4}{p_3+p_4}
                \end{bmatrix}\\
            &= \begin{bmatrix}
                   p_1 + p_3 & p_2 + p_4
                \end{bmatrix} = f_x \\
\end{aligned}
$$

## Intuición para el caso bivariado (continuo)

Partamos que conocemos $f_{X|Y}(x|y)$ y $f_{Y|X}(y|x)$. Si queremos la marginal, por definición: $f_X(x) = \int f_{xy}(x,y)dy$

Por Bayes sabemos que: $f_{xy}(x,y) = f_{X|Y}(x|y)f_Y(y)$, por tanto: $f_X(x) = \int f_{X|Y}(x|y)f_Y(y)dy$

De manera similar, tenemos $f_{xy}(x,y) = f_{Y|X}(y|x)f_X(y)$ con $f_Y(y) = \int f_{Y|X}(y|x)f_X(x)dx$. 

Reemplazando:

\[
f_X(x) = \int f_{X|Y}(x|y)\int f_{Y|X}(y|t)f_X(t)dtdy  = \int \left [ \int f_{X|Y}(x|y) f_{Y|X}(y|t) dy \right ]f_X(t)dt = \int h(x,t)f_X(t)dt
\]

con $h(x,t) = \left [ \int f_{X|Y}(x|y) f_{Y|X}(y|t) dy \right ]$. La ecuación con $f_x$ se conoce como una ecuación de punto fijo integral y la secuencia de Gibbs es una manera de converger a la solución de dicha ecuación.

Sin embargo, el algoritmo de Gibbs no siempre nos lleva a la marginal que deseamos. Veamos el siguiente ejemplo de distribuciones condicionales exponenciales definidas en $(0, \infty)$.


$$
f(x|y) =  y e^{-yx}, 0 < x < \infty \quad | \quad
f(y|x) = x e^{-xy}, 0 < y < \infty 
$$

$$
f_{X}(x) = \int \left [\int y\exp^{-yx}t\exp^{-ty}dy \right]f_X(t)dt = \int \left [\frac{t}{(x+t)^2} \right]f_X(t)dt
$$

Vemos que $f_X(t) = 1/t$ resuelve la ecuación (reemplazando). Pero cuando intentamos aplicar el muestrador de Gibbs a estas condicionales, no obtenemos convergencia (es decir, una aproximación a $1/x$), la mayor parte de los valores se acumula cerca 0:

```{r include=FALSE}
R <- 25000 #numero de repeticiones a mantener
burnin <- 1500 #observaciones a descartar
iter <- R + burnin

#inicializamos los vectores donde se guardarán las sucesiones
X_vec <- vector(mode="double", length=R)
Y_vec <- vector(mode="double", length=R)


```


```{r ejemplo-no-converge, include=FALSE}
#algoritmo de sampling visto en clase
#y <- rtrunc(1, spec = "norm", a = 0, b = B)
y <- rtrunc(1, spec = "norm", a = 0, b = Inf)
for (i in 1:iter) {
  #K_x <- 1 / (1 - exp(-B*y))
  #x <- rtrunc(1, spec = "exp", a = 0, b = B, rate = y)
  x <- rexp(1, rate = y)
  #K_y <- 1 / (1 - exp(-B*x))
  #y <- rtrunc(1, spec = "exp", a = 0, b = B, rate = x)  
  y <- rexp(1, rate = x)
  if (i > burnin) {
    X_vec[i] <- x
    Y_vec[i] <- y
  }
}

```


```{r graph2-no_acotado, fig.height=2, fig.width=5, include=FALSE}
plot1 <- data_frame(val = X_vec) %>%
          filter(val < 80) %>%
          ggplot(., aes(val)) + 
                geom_histogram( binwidth = 2)  + 
                labs(caption = "Histograma completo", x = "x", y = "Total")

```
```{r graph3-no_acotado_limitado, fig.height=3, fig.width=4, include=FALSE}
plot2 <- data_frame(val = X_vec, f = X_vec) %>%
          filter(val < 24) %>%
          ggplot(., aes(val)) + 
                geom_histogram( binwidth = 2)  +
                coord_cartesian(xlim=c(0,25), ylim=c(0, 150)) +
                labs(caption = "Zoom en el eje y", x = "x", y = "Total")
              
```

```{r fig.height=2, fig.width=5, fig.align='center'}
grid.arrange(plot1, plot2, ncol = 2)

```

Podemos evitar esto si truncamos las distribuciones condicionales de manera que se cumpla:

$$
\int_0^B f_X(x)dx < \infty
$$

Por contraposición, en el caso sin truncar la marginal no integra a un número finito: $\int_0^\infty  \frac{1}{x}dx \rightarrow \infty$

Una condición para que algoritmo converga es que la marginal sea una función de densidad $\int f_X(x)dx < \infty$. Una manera de segurar esto es limitando las condicionales.

