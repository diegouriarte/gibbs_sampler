---
title: "Muestrador de Gibbs"
author: "Diego Uriarte"
date: "14/9/2018"
output: 
  ioslides_presentation:
    smaller: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(dplyr)
library(truncdist)
```

## ¿Qué es el muestrador de Gibbs?

- Permite muestrar de una distribución sin tener que calcular la densidad.
- Está basada en cadenas de Markov
- Queremos muestrar de f(x)

$$ f(x) = \int \cdots \int f(x, y_1, \cdots, y_p) dy_1 \cdots dy_p $$

- ¿Qué podemos hacer?
- Resolver analítica o numéricamente, pero ¿si no se puede?



## ¿Qué es una cadena de Markov?


## ¿Cómo funciona el muestrador de Gibbs? (i)

- Ilustraremos con un ejemplo bivariado. Queremos muestras de $f(x)$ (distribuida de manera conjunta con $y$). Conocemos las condicionales $f(x|y)$ y $f(y|x)$.

- Se especifica un valor inicial para $Y'_0 = y'_0$. 

- Se obtiene apartir de las condicionales una sucesión de muestras para x e y:

$$
X'_j \sim f(x|Y'_j = y'_j)
$$
$$
Y'_{j+1} \sim f(y|X'_j = x'_j)
$$

## ¿Cómo funciona el muestrador de Gibbs? (ii)


- Obtenemos así una secuencia: $Y'_0, X'_0, Y'_1, X'_1,\cdots Y'_k, X'_k,$

- Resulta que bajo algunas condiciones generales, cuando $k \rightarrow \infty$, $X'_k$ es efectivamente una muestra de $f(x)$


## Aplicación I

- Tenemos X e Y que tienen distribuciones condicionales del tipo exponencial (restringidas al intervalo $(0,B)$:

$$
f(x|y) \propto y e^{-yx}, 0 < x < B < \infty \\
f(y|x) \propto x e^{-xy}, 0 < y < B < \infty 
$$

## Distribución

- Tomamos B = 5, se mantienen 25000 repeticiones y se desechan las primeras 1500.Los muestreos se realizan de distribuciones truncas exponenciales.

```{r include=FALSE, eval=FALSE}
B <- 5 #parámetro que trunca la dist. exponencial
k <- 15 #
R <- 500 #numero de repeticiones a mantener
iter <- R + burnin

#inicializamos los vectores donde se guardarán las sucesiones
X_vec <- vector(mode="double", length=R)
Y_vec <- vector(mode="double", length=R)


```

```{r eval=FALSE, include=FALSE}
for (i in c(1:R)) {
  y <- rtrunc(1, spec = "norm", a = 0, b = B)
  
  for (j in 1:k) {
    K_x <- 1 / (1 - exp(-B*y))
    x <- rtrunc(1, spec = "exp", a = 0, b = B, rate = y)
    K_y <- 1 / (1 - exp(-B*x))
    y <- rtrunc(1, spec = "exp", a = 0, b = B, rate = x)
  }
  X_vec[i] <- x
  Y_vec[i] <- y
}

```

```{r include=FALSE, eval=FALSE}
data_frame(val = X_vec) %>%
          ggplot(., aes(val)) + 
                geom_histogram(aes(y=..density..), binwidth = 0.5, colour="black", fill="white") + 
                 geom_density(alpha=.2, fill="#FF6666") 
```

```{r eval=FALSE, include=FALSE}
for (i in c(1:R)) {
  y <- rtrunc(1, spec = "norm", a = 0, b = B)
  
  for (j in 1:k) {
    u <- rtrunc(1, spec = "unif", a = -1, b = 1 / (1 - exp(-B*y)))
    x <- -log(1 - u * (1 - exp(-B*y))) / y
    u <- rtrunc(1, spec = "unif", a = -1, b = 1 / (1 - exp(-B*x)))
    y <- -log(1 - u * (1 - exp(-B*x))) / x
  }
  X_vec[i] <- x
  Y_vec[i] <- y
}

hist(X_vec)
```

```{r include=FALSE}
B <- 5
k <- 100
R <- 25000 #numero de repeticiones a mantener
burnin <- 1500 #observaciones a descartar
iter <- R + burnin
y <- -1 #inicializa el algoritmo

#inicializamos los vectores donde se guardarán las sucesiones
X_vec <- vector(mode="double", length=R)
Y_vec <- vector(mode="double", length=R)


```


```{r metodo_diego_wk, include=FALSE}
#algoritmo de sampling visto en clase
y <- rtrunc(1, spec = "norm", a = 0, b = B)
for (i in 1:iter) {
  K_x <- 1 / (1 - exp(-B*y))
  x <- rtrunc(1, spec = "exp", a = 0, b = B, rate = y)
  K_y <- 1 / (1 - exp(-B*x))
  y <- rtrunc(1, spec = "exp", a = 0, b = B, rate = x)  
  if (i > burnin) {
    X_vec[i] <- x
    Y_vec[i] <- y
  }
  }
  
```

```{r graph1_acotado, fig.height=3, fig.width=4}
data_frame(val = X_vec) %>%
          ggplot(., aes(val)) + 
                geom_histogram(aes(y=..density..), binwidth = 0.2, colour="black", fill="white") + 
                 geom_density(alpha=.2, fill="#FF6666") 
```


## Una prueba simple de la convergencia del muestrador de Gibbs

Mostraremos como converge el muestreo tomando como ejemplo una distribución bivariada, donde X e Y se distribuyen Bernoulli de manera conjunta.


<div class="centered">
![](images/01_ejemplo-distribucion-bivariada-bernoulli.jpg)
</div>

***
La distribución conjunta de X, Y está dada por:

\[
\begin{bmatrix}f_{x,y}(0,0) & f_{x,y}(1,0)\\
f_{x,y}(0,1) & f_{x,y}(1,1)
\end{bmatrix}=\begin{bmatrix}p_{1} & p_{2}\\
p_{\text{3}} & p_{4}
\end{bmatrix}
\]

La marginal de X es:

\[
f_x = \begin{bmatrix} p_1 + p_3 & p_2 + p_4
\end{bmatrix} 
\]

Determinamos las probabilidades condicionales $X|Y = y$ e $Y|X = x$, formulando las siguientes matrices:

\[
A_{y|x}=\begin{bmatrix} Prob(Y=0|X=0) & Prob(Y=1|X=0)\\
                        Prob(Y=0|X=1)& Prob(Y=1|X=1)  
\end{bmatrix}
=
\begin{bmatrix} \frac{p_1}{p_1+p_3} & \frac{p_3}{p_1+p_3}\\
                \frac{p_2}{p_2+p_4}& \frac{p_4}{p_2+p_4}
\end{bmatrix}
\]

\[
A_{x|y}=\begin{bmatrix} Prob(X=0|Y=0) & Prob(X=1|Y=0)\\
                        Prob(X=0|Y=1)& Prob(X=1|Y=1)  
\end{bmatrix}
=
\begin{bmatrix} \frac{p_1}{p_1+p_2} & \frac{p_2}{p_1+p_2}\\
                \frac{p_3}{p_3+p_4}& \frac{p_4}{p_3+p_4}
\end{bmatrix}
\]

***

Dado que estamos interesados en muestrar de la marginal de X, nos interesan los valores $X'_0, X'_1, X'_2, \cdots$. Sin embargo, para ir de $X'_0 \rightarrow X'_1$ debemos  muestrar $Y'_1$, es decir, $X'_0 \rightarrow Y'_1 \rightarrow X'_1$. 


$$
P(X'_1 = x_1 |X'_0 = x_0) = \sum_y P(X'_1 = x_1 |Y'_1 = y) \times P(Y'_1 = y_1 |X'_0 = x_0) 
$$


Para este caso en particular, se tiene que la matrices de transición para x está dada por:

\[A_{x|x} = A_{y|x} A_{x|y}\]

Ahora, es directo calcular la distribución marginal de realización k, $X'_k$:

$f_k = f_0 A^k_{x|x} = (f_0 A^{k-1}_{x|x})A_{x|x} = f_{k-1}A_{x|x}$



***

Bajo ciertas condiciones, se tiene un punto fijo tal que:

$$ f = f A_{x|x}$$

En nuestro caso simple:


$$
\begin{aligned}
f_x A_{x|x} &=  f_x A_{y|x} A_{x|y} \\
            &=  \begin{bmatrix}
                   p_1 + p_3 & p_2 + p_4
                \end{bmatrix}
                \begin{bmatrix} 
                  \frac{p_1}{p_1+p_3} & \frac{p_3}{p_1+p_3}\\
                  \frac{p_2}{p_2+p_4}& \frac{p_4}{p_2+p_4}
                \end{bmatrix} 
                \begin{bmatrix} 
                  \frac{p_1}{p_1+p_2} & \frac{p_2}{p_1+p_2}\\
                  \frac{p_3}{p_3+p_4}& \frac{p_4}{p_3+p_4}
                \end{bmatrix} \\
            &=  \begin{bmatrix}
                   p_1 + p_2 & p_3 + p_4
                \end{bmatrix}
                \begin{bmatrix} 
                  \frac{p_1}{p_1+p_2} & \frac{p_2}{p_1+p_2}\\
                  \frac{p_3}{p_3+p_4}& \frac{p_4}{p_3+p_4}
                \end{bmatrix}\\
            &= \begin{bmatrix}
                   p_1 + p_3 & p_2 + p_4
                \end{bmatrix} \\
            &= f_x 
\end{aligned}
$$



## Intuición para el caso bivariado

-> ¿Cómo es que podemos llegar a partir de las condicionales a la marginal? 

- Conocemos $f_{X|Y}(x|y)$ y $f_{Y|X}(y|x)$. Si queremos la marginal, por definición:

$$
f_X(x) = \int f_{xy}(x,y)dy
$$
- Por Bayes sabemos que: $f_{xy}(x,y) = f_{X|Y}(x|y)f_Y(y)$, por tanto:

$$
f_X(x) = \int f_{X|Y}(x|y)f_Y(y)dy
$$
De manera similar, tenemos $f_{xy}(x,y) = f_{Y|X}(y|x)f_X(y)$ con $f_Y(y) = \int f_{Y|X}(y|x)f_X(x)dx$. Reemplazando:

$$
\begin{aligned}
f_X(x) &= \int f_{X|Y}(x|y)\int f_{Y|X}(y|t)f_X(t)dtdy \\
       &= \int \left [ \int f_{X|Y}(x|y) f_{Y|X}(y|t) dy \right ]f_X(t)dt \\
       &= \int h(x,t)f_X(t)dt
\end{aligned}
$$

con $h(x,t) = \left [ \int f_{X|Y}(x|y) f_{Y|X}(y|t) dy \right ]$.

- Llegamos a lo que se conoce como una ecuación de punto fijo integral.

- La secuencia de Gibbs es una manera de converger a la solución de la ecuación.

***

¿Siempre tiene solución?

Veamos el primer ejemplo, pero sin acotar las distribuciones condicionales.


$$
f(x|y) =  y e^{-yx}, 0 < x < \infty \\
f(y|x) = x e^{-xy}, 0 < y < \infty 
$$
$$
\begin{aligned}
f_{X}(x) &= \int \left [\int y\exp^{-yx}t\exp^{-ty}dy \right]f_X(t)dt \\
         &= \int \left [\frac{t}{(x+t)^2} \right]f_X(t)dt
\end{aligned}
$$
Y notamos que $f_X(t) = 1/t$ es solución puesto que:

$$
\begin{aligned}
\frac{1}{x} &= \int_0^\infty \left [\frac{t}{(x+t)^2} \right]\frac{1}{t}dt \\
            &= \int_0^\infty \left [\frac{1}{(x+t)^2} \right]dt \\
            &=  \left [\frac{-1}{(x+t)} \right]_{t=0}^{t=\infty} \\
            &=   0 -   \frac{-1}{x} = \frac{1}{x}\\
\end{aligned}
$$
***

Pero cuando intentamos aplicar el muestrador de Gibbs a estas condicionales, no obtenemos convergencia:

```{r include=FALSE}
R <- 25000 #numero de repeticiones a mantener
burnin <- 1500 #observaciones a descartar
iter <- R + burnin

#inicializamos los vectores donde se guardarán las sucesiones
X_vec <- vector(mode="double", length=R)
Y_vec <- vector(mode="double", length=R)


```


```{r metodo_diego_wk, include=FALSE}
#algoritmo de sampling visto en clase
#y <- rtrunc(1, spec = "norm", a = 0, b = B)
y <- rtrunc(1, spec = "norm", a = 0, b = Inf)
for (i in 1:iter) {
  #K_x <- 1 / (1 - exp(-B*y))
  #x <- rtrunc(1, spec = "exp", a = 0, b = B, rate = y)
  x <- rexp(1, rate = y)
  #K_y <- 1 / (1 - exp(-B*x))
  #y <- rtrunc(1, spec = "exp", a = 0, b = B, rate = x)  
  y <- rexp(1, rate = x)
  if (i > burnin) {
    X_vec[i] <- x
    Y_vec[i] <- y
  }
}

f_x <- 1 / X_vec
```


```{r graph2-no_acotado, fig.height=3, fig.width=4}
data_frame(val = X_vec) %>%
          filter(val < 80) %>%
          ggplot(., aes(val)) + 
                geom_histogram( binwidth = 2)  
#falta graficar 1/x            
    
```

```{r graph3-no_acotado_limitado, fig.height=3, fig.width=4}
x_graph <- 1:2500/100
y_graph <- 1 / x_graph

data_frame(val = X_vec, f = X_vec) %>%
          filter(val < 24) %>%
          ggplot(., aes(val)) + 
                geom_histogram( binwidth = 2)  +
                coord_cartesian(xlim=c(0,25), ylim=c(0, 150)) +
                geom_line(data = data_frame(x_graph, y_graph), aes(x = x_graph, y = y_graph))
#falta graficar 1/x            



```

Vemos que ahora el procedimiento de Gibbs no converge puesto que x toma valores cada vez más grandes.

Podemos evitar esto si truncamos las distribuciones condicionales de manera que se cumpla:

$$
\int_0^B f_X(x)dx < \infty
$$

Vemos que en caso sin truncar, las marginal no integra a un número finito: $\int_0^\infty  \frac{1}{x}dx \rightarrow \infty$

Una condición para que algoritmo converga es que la marginal sea una función de densidad $\int f_X(x)dx < \infty$. Una manera de segurar esto es limitando las condicionales.

